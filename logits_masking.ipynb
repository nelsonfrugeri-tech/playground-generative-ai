{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNat1EJJ9+sn6+bdcYHrD+v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelsonfrugeri-tech/playground-generative-ai/blob/master/logits_masking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)\n"
      ],
      "metadata": {
        "id": "8OICT8fUUa94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2Bd4wZ3WoF7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Deve imprimir: True\n",
        "print(torch.cuda.get_device_name(0))  # Deve mostrar: 'NVIDIA A100-SXM4-40GB'\n"
      ],
      "metadata": {
        "id": "56hCGixKSiU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"google/gemma-3-4b-pt\""
      ],
      "metadata": {
        "id": "zycsJ1MvEIQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LogitsProcessor, pipeline\n",
        "from transformers.utils import add_start_docstrings\n",
        "from transformers.generation.logits_process import LOGITS_PROCESSOR_INPUTS_DOCSTRING\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xmK5eD2SELAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RulesLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, tokenizer, palavras_proibidas):\n",
        "        self.tokenizer = tokenizer\n",
        "        # Converte cada palavra proibida em lista de token_ids\n",
        "        self.tokens_proibidos = set()\n",
        "        for palavra in palavras_proibidas:\n",
        "            for tid in tokenizer.encode(palavra, add_special_tokens=False):\n",
        "                self.tokens_proibidos.add(tid)\n",
        "\n",
        "    def apply_rules(self, seq):\n",
        "        return not any(palavra in seq for palavra in palavras_proibidas)\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, input_logits: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        output_logits = input_logits.clone()\n",
        "        eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        for idx, input_id in enumerate(input_ids):\n",
        "            seq = self.tokenizer.decode(input_id)\n",
        "            if not self.apply_rules(seq):\n",
        "                # Permite sempre ao menos o EOS\n",
        "                output_logits[idx] = -float(\"inf\")\n",
        "                output_logits[idx, eos_token_id] = 0\n",
        "\n",
        "        # Também bloqueia qualquer próximo token proibido:\n",
        "        output_logits[:, list(self.tokens_proibidos)] = -float(\"inf\")\n",
        "        return output_logits"
      ],
      "metadata": {
        "id": "3KPfeb_OENnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega modelo e tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)"
      ],
      "metadata": {
        "id": "4eEumEowEPnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defina uma lista de palavras proibidas para a regra (exemplo simples)\n",
        "palavras_proibidas = [\"desculpe\", \"infelizmente\"]\n",
        "\n",
        "# Cria pipeline\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        ")\n",
        "\n",
        "rules_processor = MyRulesLogitsProcessor(tokenizer, palavras_proibidas)\n",
        "\n",
        "# Defina o prompt de entrada\n",
        "input_message = (\n",
        "    \"<|system|>\\n\"\n",
        "    \"Você é um atendente cordial, direto e focado na solução. \"\n",
        "    \"Explique ao cliente o que será feito para resolver o problema. Seja transparente e objetivo. \\n\\n\"\n",
        "    \"Abaixo segue informações do sistema de logística. \\n\\n\"\n",
        "    \"Consulta ao sistema logístico: status do pedido #12345 — Pedido ATRASADO. Motivo: alto volume de entregas na região. Previsão de nova entrega: 2 dias úteis.\\n\\n\"\n",
        "    \"<|user|>\\n\"\n",
        "    \"Meu pedido está atrasado e eu preciso de uma solução urgente. Não aceito mais desculpas, quero uma resposta clara e objetiva sobre o que será feito.\\n\"\n",
        "    \"<|assistant|>\\n\"\n",
        ")\n",
        "\n",
        "\n",
        "# Executa a geração\n",
        "results = pipe(\n",
        "    input_message,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=True,\n",
        "    temperature=1.0,\n",
        "    num_beams=5,\n",
        "    logits_processor=[rules_processor],\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"Resposta gerada:\\n\", results)"
      ],
      "metadata": {
        "id": "3Ud9ByEVNZVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "from abc import ABC, abstractmethod\n",
        "from transformers import LogitsProcessor, PreTrainedTokenizerBase\n",
        "import torch\n",
        "\n",
        "\n",
        "class MaskingRules(ABC):\n",
        "    @abstractmethod\n",
        "    def get_tokenid2prob(self) -> Dict[int, float]:\n",
        "      pass\n",
        "\n",
        "\n",
        "class MaskingRulesImpl(MaskingRules):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizerBase, regras_dict: Dict[str, float]):\n",
        "        self.tokenid2prob: Dict[int, float] = {}\n",
        "        for termo, prob in regras_dict.items():\n",
        "            token_ids = tokenizer.encode(termo, add_special_tokens=False)\n",
        "            for tid in token_ids:\n",
        "                self.tokenid2prob[tid] = -float(\"inf\") if prob == 0 else float(prob)\n",
        "\n",
        "    def get_tokenid2prob(self) -> Dict[int, float]:\n",
        "        return self.tokenid2prob\n",
        "\n",
        "\n",
        "class RulesLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, masking_rules: MaskingRules):\n",
        "        self.tokenid2prob: Dict[int, float] = masking_rules.get_tokenid2prob()\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        scores: torch.FloatTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "        output_logits: torch.FloatTensor = scores.clone()\n",
        "        for tid, prob in self.tokenid2prob.items():\n",
        "            output_logits[:, tid] += prob\n",
        "        return output_logits\n"
      ],
      "metadata": {
        "id": "MiLMDFHAI1mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "from transformers import pipeline, PreTrainedTokenizerBase, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Defina regras com dicionário: termo -> probabilidade\n",
        "rules: Dict[str, float] = {\n",
        "    \"New Balance\": 3,    # Incentiva fortemente a marca New Balance\n",
        "    \"Asics\": 2,          # Incentiva fortemente a marca Asics\n",
        "    \"Adidas\": -3,        # Desincentiva Adidas, menor relevância\n",
        "    \"Nike\": -3           # Desincentiva Nike, menor relevância\n",
        "}\n",
        "\n",
        "# Instancia MaskingRules e o processor\n",
        "rules_processor = RulesLogitsProcessor(MaskingRulesImpl(tokenizer, rules))\n",
        "\n",
        "# Cria pipeline Hugging Face\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        ")\n",
        "\n",
        "# Prompt realista de compliance regulatório financeiro\n",
        "input_message = (\n",
        "    \"\"\"\n",
        "    <bos><start_of_turn>system\n",
        "    Você é um assistente virtual especializado em recomendar tênis esportivos para clientes de e-commerce.\n",
        "    Recomende os tênis do nosso catálogo:\n",
        "    Nike\n",
        "    Adidas\n",
        "    Asics\n",
        "    New Balance\n",
        "    Puma\n",
        "    Under Armour\n",
        "    Reebok\n",
        "    Mizuno\n",
        "    Saucony\n",
        "    Brooks\n",
        "    ---\n",
        "    De acordo com seus conhecimentos recomende o melhor tênis dentro do nosso catálogo para o cliente.\n",
        "    Escreva na lingua portuguesa do Brasil.\n",
        "    Gere uma resposta em linguagem natural, lembre-se que você está conversando com um humano.\n",
        "    <end_of_turn>\n",
        "    <start_of_turn>user\n",
        "    Quero comprar um bom tênis de corrida para longas distâncias, pode me recomendar algumas opções?\n",
        "    <end_of_turn>\n",
        "    <start_of_turn>model\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Executa a geração\n",
        "results = pipe(\n",
        "    input_message,\n",
        "    max_new_tokens=110,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    num_beams=5,\n",
        "    logits_processor=[rules_processor],\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"Resposta gerada:\\n\", results[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "QbjPDx96I-GZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}