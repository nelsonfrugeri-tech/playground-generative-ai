{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1 – Autenticação no Hugging Face Hub\n",
        "\n",
        "Este trecho importa a função login do SDK Hugging Face Hub e abre uma sessão autenticada.\n",
        "\t•\tnew_session=False indica que, se já houver um token válido na máquina (variável HUGGINGFACE_HUB_TOKEN ou cache local), ele será reutilizado, evitando múltiplas autenticações interativas.\n",
        "\t•\tA autenticação é necessária para baixar modelos privados ou com cotas restritas diretamente dos servidores da Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OICT8fUUa94"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2 – Instalação das dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dependencies installation\n",
        "!pip install torch>=2.0.0 transformers>=4.30.0 huggingface_hub>=0.15.0 numpy>=1.21.0 accelerate>=0.20.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelsonfrugeri-tech/playground-generative-ai/blob/master/logits_masking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3 – Verificação de GPU (CUDA)\n",
        "Este bloco confirma se o PyTorch reconhece uma GPU CUDA e exibe o nome do dispositivo 0.\n",
        "\t•\ttorch.cuda.is_available() retorna True quando há driver CUDA e GPU compatível.\n",
        "\t•\ttorch.cuda.get_device_name(0) mostra o modelo da placa (neste caso, A100).\n",
        "\n",
        "Essas checagens evitam erros futuros—por exemplo, carregar o modelo em GPU quando a infraestrutura não oferece suporte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56hCGixKSiU1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Deve imprimir: True\n",
        "print(torch.cuda.get_device_name(0))  # Deve mostrar: 'NVIDIA A100-SXM4-40GB'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4 – Seleção do Modelo\n",
        "\n",
        "Define uma constante com o identificador do modelo no Hub. google/gemma-3-4b-pt é:\n",
        "\t•\tUma variante de 4 bilhões de parâmetros da família Gemma 3.\n",
        "\t•\tSufixo -pt indica que o checkpoint tem suporte (ou foco) para português.\n",
        "\n",
        "Esse ID será reutilizado mais adiante para carregar AutoTokenizer e AutoModelForCausalLM, mantendo o código limpo e configurável."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zycsJ1MvEIQg"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"google/gemma-3-4b-pt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5 – Importações Principais\n",
        "\n",
        "Este bloco reúne todas as dependências essenciais para geração de texto com controle de logits:\n",
        "| **Componente**                                         | **Função no notebook**                                                                                                                     |\n",
        "|--------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| `torch`                                                | Backend tensorial que executa operações em CPU ou GPU.                                                                                     |\n",
        "| `pipeline`                                             | *Wrapper* de alto nível para inferência; simplifica o carregamento do modelo e do tokenizer.                                               |\n",
        "| `LogitsProcessor`                                      | Classe-base para criar processadores que alteram logits antes do *sampling*.                                                               |\n",
        "| `PreTrainedTokenizerBase`                              | Classe-base para todos os tokenizers pré-treinados; define a interface comum de **encode/decode** e o manuseio de vocabulário especial.    |\n",
        "| `AutoTokenizer`, `AutoModelForCausalLM`                | Carregam automaticamente o tokenizer e o modelo correspondentes ao `MODEL_ID`.                                                             |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmK5eD2SELAE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline, LogitsProcessor, PreTrainedTokenizerBase, AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5 - Classe `RulesLogitsProcessor`: Logits Masking Customizado para Tokens Proibidos\n",
        "\n",
        "Esta classe personalizada de `LogitsProcessor` permite aplicar **Logits Masking** durante a geração de texto por modelos de linguagem (LLMs) usando Hugging Face Transformers.  \n",
        "O objetivo é **bloquear a geração de palavras ou termos proibidos** (compliance, ética, branding etc) já no momento do sampling, evitando que o modelo produza saídas indesejadas.\n",
        "\n",
        "- **`__init__`**: Recebe o tokenizer e uma lista de palavras proibidas, convertendo cada uma delas nos respectivos token_ids para aplicação eficiente do masking.\n",
        "- **`apply_rules`**: Função utilitária que retorna `False` se qualquer palavra proibida já aparecer na sequência gerada até o momento.\n",
        "- **`__call__`**: Método obrigatório do `LogitsProcessor`, chamado a cada etapa de geração.  \n",
        "  - Zera a probabilidade (`-inf`) dos tokens proibidos, garantindo que não possam ser gerados.\n",
        "  - Garante que o token especial EOS (fim de sequência) nunca seja bloqueado, evitando loops de geração.\n",
        "  - Bloqueia explicitamente qualquer próximo token proibido na matriz de logits.\n",
        "\n",
        "**Use Cases:**  \n",
        "- Compliance regulatório\n",
        "- Prevenção de respostas ofensivas\n",
        "- Alinhamento a políticas internas de comunicação ou produto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KPfeb_OENnt"
      },
      "outputs": [],
      "source": [
        "class RulesLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, tokenizer, palavras_proibidas):\n",
        "        self.tokenizer = tokenizer\n",
        "        # Converte cada palavra proibida em lista de token_ids\n",
        "        self.tokens_proibidos = set()\n",
        "        for palavra in palavras_proibidas:\n",
        "            for tid in tokenizer.encode(palavra, add_special_tokens=False):\n",
        "                self.tokens_proibidos.add(tid)\n",
        "\n",
        "    def apply_rules(self, seq):\n",
        "        return not any(palavra in seq for palavra in self.palavras_proibidas)\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, input_logits: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        output_logits = input_logits.clone()\n",
        "        eos_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "        for idx, input_id in enumerate(input_ids):\n",
        "            seq = self.tokenizer.decode(input_id)\n",
        "            if not self.apply_rules(seq):\n",
        "                # Permite sempre ao menos o EOS\n",
        "                output_logits[idx] = -float(\"inf\")\n",
        "                output_logits[idx, eos_token_id] = 0\n",
        "\n",
        "        # Também bloqueia qualquer próximo token proibido:\n",
        "        output_logits[:, list(self.tokens_proibidos)] = -float(\"inf\")\n",
        "        return output_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6 - Carregamento do Modelo e Tokenizer\n",
        "\n",
        "Neste trecho, o modelo de linguagem (LLM) e seu tokenizer correspondente são carregados a partir do Hugging Face Hub utilizando o identificador definido em `MODEL_ID`.  \n",
        "- **`AutoTokenizer.from_pretrained(MODEL_ID)`**: Baixa e instancia o tokenizer apropriado para o modelo, responsável por converter texto em tokens e vice-versa.\n",
        "- **`AutoModelForCausalLM.from_pretrained(MODEL_ID)`**: Baixa e instancia o modelo de linguagem pré-treinado, preparado para tarefas de geração textual (`causal language modeling`).\n",
        "\n",
        "Esse passo é fundamental para configurar o pipeline de geração de texto com o modelo desejado, garantindo compatibilidade entre tokenizer e modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eEumEowEPnY"
      },
      "outputs": [],
      "source": [
        "# Carrega modelo e tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7 - Pipeline de Geração com Logits Masking e Palavras Proibidas\n",
        "\n",
        "Neste trecho, é configurado o pipeline completo para geração de texto com bloqueio de termos indesejados:\n",
        "\n",
        "- **Definição de palavras proibidas:**  \n",
        "  Uma lista simples (`palavras_proibidas`) contém termos que devem ser evitados nas respostas geradas pelo modelo.\n",
        "\n",
        "- **Pipeline Hugging Face:**  \n",
        "  O pipeline é criado para tarefa de geração de texto (`text-generation`), conectando o modelo, tokenizer e configurando o uso de GPU se disponível.\n",
        "\n",
        "- **Aplicação do Logits Masking:**  \n",
        "  O `rules_processor` (`MyRulesLogitsProcessor`) recebe o tokenizer e as palavras proibidas, sendo passado ao pipeline via `logits_processor`, garantindo que as palavras bloqueadas nunca apareçam nas respostas.\n",
        "\n",
        "- **Prompt estruturado:**  \n",
        "  O `input_message` simula um cenário de atendimento ao cliente, com informações do sistema logístico e a pergunta do usuário.\n",
        "\n",
        "- **Execução da geração:**  \n",
        "  O pipeline gera a resposta considerando as restrições definidas, maximizando a chance de respostas adequadas, objetivas e dentro das regras especificadas.\n",
        "\n",
        "Esse setup é ideal para cenários onde compliance e experiência do usuário exigem controle total sobre o que pode ou não ser gerado pelo assistente virtual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ud9ByEVNZVD"
      },
      "outputs": [],
      "source": [
        "# Defina uma lista de palavras proibidas para a regra (exemplo simples)\n",
        "palavras_proibidas = [\"desculpe\", \"infelizmente\"]\n",
        "\n",
        "# Cria pipeline\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        ")\n",
        "\n",
        "rules_processor = MyRulesLogitsProcessor(tokenizer, palavras_proibidas)\n",
        "\n",
        "# Defina o prompt de entrada\n",
        "input_message = (\n",
        "    \"<|system|>\\n\"\n",
        "    \"Você é um atendente cordial, direto e focado na solução. \"\n",
        "    \"Explique ao cliente o que será feito para resolver o problema. Seja transparente e objetivo. \\n\\n\"\n",
        "    \"Abaixo segue informações do sistema de logística. \\n\\n\"\n",
        "    \"Consulta ao sistema logístico: status do pedido #12345 — Pedido ATRASADO. Motivo: alto volume de entregas na região. Previsão de nova entrega: 2 dias úteis.\\n\\n\"\n",
        "    \"<|user|>\\n\"\n",
        "    \"Meu pedido está atrasado e eu preciso de uma solução urgente. Não aceito mais desculpas, quero uma resposta clara e objetiva sobre o que será feito.\\n\"\n",
        "    \"<|assistant|>\\n\"\n",
        ")\n",
        "\n",
        "\n",
        "# Executa a geração\n",
        "results = pipe(\n",
        "    input_message,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=True,\n",
        "    temperature=1.0,\n",
        "    num_beams=5,\n",
        "    logits_processor=[rules_processor],\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"Resposta gerada:\\n\", results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8 - Evolução na classe RulesLogitsProcessor - Estrutura Modular para Logits Masking — Interface, Implementação e Processor\n",
        "\n",
        "Este bloco define uma arquitetura modular e extensível para aplicação de Logits Masking em modelos de linguagem, usando princípios de programação orientada a interfaces:\n",
        "\n",
        "- **`MaskingRules` (Interface/ABC):**  \n",
        "  Uma interface abstrata para a definição de regras de masking, exigindo a implementação do método `get_tokenid2prob`, que retorna um dicionário mapeando token IDs para suas probabilidades ajustadas.\n",
        "\n",
        "- **`MaskingRulesImpl` (Implementação):**  \n",
        "  Implementação concreta da interface `MaskingRules`.  \n",
        "  - No construtor, converte cada termo e sua probabilidade em token IDs.  \n",
        "  - Probabilidade 0 é automaticamente convertida para bloqueio total (`-inf`), outras probabilidades são aplicadas conforme desejado.\n",
        "\n",
        "- **`RulesLogitsProcessor` (Custom LogitsProcessor):**  \n",
        "  Classe personalizada que utiliza qualquer implementação de `MaskingRules` para aplicar logits masking durante a geração de texto.  \n",
        "  - No método `__call__`, para cada token configurado, ajusta seu logit somando a probabilidade especificada, permitindo tanto bloqueio total quanto incentivo/desincentivo sutil.\n",
        "\n",
        "**Benefícios deste design:**\n",
        "- Separação clara de responsabilidades (regras vs. aplicação).\n",
        "- Facilidade para testar, expandir ou trocar a lógica de regras.\n",
        "- Pronto para cenários de compliance, branding, priorização de produtos e segurança textual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiLMDFHAI1mN"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "class MaskingRules(ABC):\n",
        "    @abstractmethod\n",
        "    def get_tokenid2prob(self) -> Dict[int, float]:\n",
        "      pass\n",
        "\n",
        "\n",
        "class MaskingRulesImpl(MaskingRules):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizerBase, regras_dict: Dict[str, float]):\n",
        "        self.tokenid2prob: Dict[int, float] = {}\n",
        "        for termo, prob in regras_dict.items():\n",
        "            token_ids = tokenizer.encode(termo, add_special_tokens=False)\n",
        "            for tid in token_ids:\n",
        "                self.tokenid2prob[tid] = -float(\"inf\") if prob == 0 else float(prob)\n",
        "\n",
        "    def get_tokenid2prob(self) -> Dict[int, float]:\n",
        "        return self.tokenid2prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RulesLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, masking_rules: MaskingRules):\n",
        "        self.tokenid2prob: Dict[int, float] = masking_rules.get_tokenid2prob()\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        scores: torch.FloatTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "        output_logits: torch.FloatTensor = scores.clone()\n",
        "        for tid, prob in self.tokenid2prob.items():\n",
        "            output_logits[:, tid] += prob\n",
        "        return output_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9 - Exemplo Prático: Priorização de Marcas em Recomendação de E-commerce com Logits Masking\n",
        "\n",
        "Este bloco de código demonstra como usar o `RulesLogitsProcessor` para **influenciar a preferência do modelo** ao recomendar produtos (marcas de tênis esportivos) em um cenário de e-commerce:\n",
        "\n",
        "- **Definição de regras de relevância:**  \n",
        "  O dicionário `rules` associa cada marca a um valor de incentivo (`3` para New Balance, `2` para Asics) ou desincentivo (`-3` para Adidas e Nike).  \n",
        "  Isso controla a probabilidade dessas marcas aparecerem nas respostas geradas pelo modelo.\n",
        "\n",
        "- **Instanciação do processor:**  \n",
        "  O `RulesLogitsProcessor` recebe uma implementação de regras (`MaskingRulesImpl`), convertendo palavras em token IDs e aplicando os incentivos/desincentivos diretamente nos logits durante a geração.\n",
        "\n",
        "- **Pipeline de geração:**  \n",
        "  O pipeline Hugging Face é criado com modelo, tokenizer e device.\n",
        "\n",
        "- **Prompt de recomendação:**  \n",
        "  O prompt orienta o assistente virtual a recomendar tênis do catálogo, em linguagem natural e em português do Brasil.\n",
        "\n",
        "- **Execução e resultado:**  \n",
        "  O modelo gera a resposta, priorizando marcas incentivadas e reduzindo menções às desincentivadas — facilitando testes e aplicações reais de personalização em sistemas de recomendação.\n",
        "\n",
        "Este setup ilustra um uso real e mensurável de Logits Masking para estratégias de negócio em sistemas de IA conversacional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbjPDx96I-GZ"
      },
      "outputs": [],
      "source": [
        "# Defina regras com dicionário: termo -> probabilidade\n",
        "rules: Dict[str, float] = {\n",
        "    \"New Balance\": 3,    # Incentiva fortemente a marca New Balance\n",
        "    \"Asics\": 2,          # Incentiva a marca Asics\n",
        "    \"Adidas\": -2,        # Desincentiva Adidas, menor relevância\n",
        "    \"Nike\": -3           # Desincentiva Nike, menor relevância\n",
        "}\n",
        "\n",
        "# Instancia MaskingRules e o processor\n",
        "rules_processor = RulesLogitsProcessor(MaskingRulesImpl(tokenizer, rules))\n",
        "\n",
        "# Cria pipeline Hugging Face\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        ")\n",
        "\n",
        "# Prompt realista de compliance regulatório financeiro\n",
        "input_message = (\n",
        "    \"\"\"\n",
        "    <bos><start_of_turn>system\n",
        "    Você é um assistente virtual especializado em recomendar tênis esportivos para clientes de e-commerce.\n",
        "    Recomende os tênis do nosso catálogo:\n",
        "    Nike\n",
        "    Adidas\n",
        "    Asics\n",
        "    New Balance\n",
        "    Puma\n",
        "    Under Armour\n",
        "    Reebok\n",
        "    Mizuno\n",
        "    Saucony\n",
        "    Brooks\n",
        "    ---\n",
        "    De acordo com seus conhecimentos recomende o melhor tênis dentro do nosso catálogo para o cliente.\n",
        "    Escreva na lingua portuguesa do Brasil.\n",
        "    Gere uma resposta em linguagem natural, lembre-se que você está conversando com um humano.\n",
        "    <end_of_turn>\n",
        "    <start_of_turn>user\n",
        "    Quero comprar um bom tênis de corrida para longas distâncias, pode me recomendar algumas opções?\n",
        "    <end_of_turn>\n",
        "    <start_of_turn>model\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Executa a geração\n",
        "results = pipe(\n",
        "    input_message,\n",
        "    max_new_tokens=110,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    num_beams=5,\n",
        "    logits_processor=[rules_processor],\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"Resposta gerada:\\n\", results[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10 — **RegexMaskingRules** para Bloquear CPFs\n",
        "Impedir que o modelo revele um CPF completo durante a geração, aplicando máscara de *logits* via regex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Dict, Pattern\n",
        "from transformers import PreTrainedTokenizerBase\n",
        "\n",
        "\n",
        "class RegexMaskingRules(MaskingRules):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizerBase, regex_patterns: Dict[str, float]):\n",
        "        self.patterns: Dict[Pattern, float] = {re.compile(pat): prob for pat, prob in regex_patterns.items()}\n",
        "        self.tokenid2prob: Dict[int, float] = {}\n",
        "\n",
        "        for token, tid in tokenizer.get_vocab().items():\n",
        "            for pattern, prob in self.patterns.items():\n",
        "                if pattern.search(token):                     # ← mudou aqui\n",
        "                    self.tokenid2prob[tid] = -float(\"inf\") if prob == 0 else float(prob)\n",
        "\n",
        "    def get_tokenid2prob(self) -> Dict[int, float]:\n",
        "        return self.tokenid2prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- CPF Regex: bloqueia qualquer token exatamente no formato de CPF ---\n",
        "cpf_regex = {r\"\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}\": 0}\n",
        "\n",
        "# Instancia RegexMaskingRules e processor\n",
        "masking_rules = RegexMaskingRules(tokenizer, cpf_regex)\n",
        "rules_processor = RulesLogitsProcessor(masking_rules)\n",
        "\n",
        "# Cria pipeline Hugging Face\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        ")\n",
        "\n",
        "# Prompt de e-commerce realista, simulando possível vazamento ou inserção de CPF\n",
        "input_message = (\n",
        "    \"\"\"<bos><start_of_turn>system\n",
        "    Você é um assistente de atendimento ao cliente. \n",
        "    Dados públicos do cliente:\n",
        "    - Nome: João da Silva\n",
        "    - ID: 873452\n",
        "    - Telefone: (11) 99999-9999    \n",
        "    - Endereço: Rua das Flores, 100 – São Paulo/SP         \n",
        "    Se precisar identificar o cliente, forneça apenas Nome e ID.\n",
        "    Jamais divulgue informações privadas e sensíveis (PII e LGPD).\n",
        "    Se precisar da identificação do cliente use o ID.\n",
        "    <end_of_turn>\n",
        "    <start_of_turn>user\n",
        "    Preciso do CPF do cliente João da Silva para finalizar um formulário. Pode me passar, por favor?\n",
        "    <end_of_turn>\n",
        "    <start_of_turn>assistant\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "# Executa a geração\n",
        "results = pipe(\n",
        "    input_message,\n",
        "    max_new_tokens=110,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    logits_processor=[rules_processor],\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"Resposta gerada:\\n\", results[0][\"generated_text\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNat1EJJ9+sn6+bdcYHrD+v",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
