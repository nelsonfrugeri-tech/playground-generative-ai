{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üéØ Logits Masking - Controlando a Gera√ß√£o de Texto\n",
        "\n",
        "Este notebook demonstra como usar **Logits Masking** para controlar a gera√ß√£o de texto em modelos de linguagem.\n",
        "\n",
        "## ü§î O que √© Logits Masking?\n",
        "\n",
        "**Logits** s√£o as probabilidades brutas que o modelo calcula para cada token (palavra) antes de escolher qual palavra gerar pr√≥xima.\n",
        "\n",
        "**Masking** significa \"mascarar\" ou modificar essas probabilidades para:\n",
        "- ‚ùå **Bloquear** palavras indesejadas (probabilidade = 0)\n",
        "- ‚úÖ **Incentivar** palavras espec√≠ficas (aumentar probabilidade)\n",
        "- üéõÔ∏è **Controlar** o comportamento do modelo\n",
        "\n",
        "## üìã Depend√™ncias Necess√°rias\n",
        "\n",
        "Antes de come√ßar, vamos instalar as bibliotecas necess√°rias:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instala√ß√£o das depend√™ncias\n",
        "!pip install torch>=2.0.0 transformers>=4.30.0 huggingface_hub>=0.15.0 numpy>=1.21.0 accelerate>=0.20.0\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîê Autentica√ß√£o no Hugging Face\n",
        "\n",
        "Para usar modelos do Hugging Face, precisamos fazer login com nossa conta.\n",
        "\n",
        "**Por que isso √© necess√°rio?**\n",
        "- Alguns modelos requerem aceitar termos de uso\n",
        "- Permite acesso a modelos privados/restritos  \n",
        "- Evita limites de download\n",
        "\n",
        "**Como obter token:**\n",
        "1. Crie conta em [huggingface.co](https://huggingface.co)\n",
        "2. V√° em Settings ‚Üí Access Tokens\n",
        "3. Crie um novo token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login no Hugging Face - Cole seu token quando solicitado\n",
        "from huggingface_hub import login\n",
        "login(new_session=False)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéÆ Verifica√ß√£o de GPU\n",
        "\n",
        "Vamos verificar se temos uma GPU dispon√≠vel para acelerar o processamento.\n",
        "\n",
        "**Por que GPU √© importante?**\n",
        "- Modelos de linguagem s√£o computacionalmente pesados\n",
        "- GPU acelera o processamento em 10-100x comparado √† CPU\n",
        "- Permite usar modelos maiores e mais poderosos\n",
        "\n",
        "**O que esperamos ver:**\n",
        "- ‚úÖ `True` = GPU dispon√≠vel\n",
        "- ‚ùå `False` = Apenas CPU (ainda funciona, mas mais lento)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica√ß√£o de GPU dispon√≠vel\n",
        "import torch\n",
        "\n",
        "print(\"üîç Verificando hardware dispon√≠vel...\")\n",
        "print(f\"GPU dispon√≠vel: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ Nome da GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ Mem√≥ria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Executando em CPU - processo ser√° mais lento\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ü§ñ Sele√ß√£o do Modelo\n",
        "\n",
        "Vamos usar o **Gemma-3-4B-PT**, um modelo de linguagem do Google.\n",
        "\n",
        "**Por que este modelo?**\n",
        "- üáßüá∑ **Portugu√™s nativo** - Treinado especificamente em portugu√™s\n",
        "- ‚ö° **Tamanho m√©dio** - 4B par√¢metros (equilibrio entre qualidade e velocidade)\n",
        "- üÜì **Gratuito** - Dispon√≠vel para uso sem restri√ß√µes\n",
        "- üéØ **Conversacional** - Bom para di√°logos e atendimento\n",
        "\n",
        "**Outros modelos que poderiam ser usados:**\n",
        "- `meta-llama/Llama-2-7b-chat-hf` (ingl√™s)\n",
        "- `microsoft/DialoGPT-medium` (ingl√™s)\n",
        "- `neuralmind/bert-base-portuguese-cased` (portugu√™s, menor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definindo o modelo que vamos usar\n",
        "MODEL_ID = \"google/gemma-3-4b-pt\"\n",
        "print(f\"üìã Modelo selecionado: {MODEL_ID}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìö Importa√ß√µes e Prepara√ß√£o\n",
        "\n",
        "Agora vamos importar todas as bibliotecas necess√°rias para implementar o Logits Masking.\n",
        "\n",
        "**O que cada biblioteca faz:**\n",
        "- üß† **transformers** - Cont√©m os modelos de linguagem e ferramentas\n",
        "- üîß **LogitsProcessor** - Classe base para processar logits (nosso ponto de entrada)\n",
        "- ü§ñ **pipeline** - Interface simples para usar modelos\n",
        "- üî¢ **torch** - Framework de deep learning (opera√ß√µes tensoriais)\n",
        "- üìä **numpy** - Opera√ß√µes matem√°ticas e arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa√ß√µes necess√°rias\n",
        "from transformers import (\n",
        "    LogitsProcessor,           # Classe base para processar logits\n",
        "    pipeline,                  # Interface simples para modelos\n",
        "    AutoTokenizer,             # Conversor texto ‚Üî n√∫meros\n",
        "    AutoModelForCausalLM       # Modelo de linguagem\n",
        ")\n",
        "import torch                   # Framework de deep learning\n",
        "import numpy as np             # Opera√ß√µes num√©ricas\n",
        "from typing import Dict        # Type hints para dicion√°rios\n",
        "\n",
        "print(\"‚úÖ Todas as importa√ß√µes carregadas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîÑ Carregando Modelo e Tokenizer\n",
        "\n",
        "Agora vamos carregar o modelo e o tokenizer do Hugging Face.\n",
        "\n",
        "**O que √© cada componente:**\n",
        "- üî§ **Tokenizer** - Converte texto em n√∫meros que o modelo entende\n",
        "  - Exemplo: \"Ol√° mundo\" ‚Üí [123, 456, 789]\n",
        "- üß† **Model** - O modelo de linguagem que gera texto\n",
        "  - Recebe n√∫meros ‚Üí Calcula probabilidades ‚Üí Gera pr√≥xima palavra\n",
        "\n",
        "**Por que carregar separadamente?**\n",
        "- Controle mais fino sobre cada componente\n",
        "- Permite usar tokenizer personalizado se necess√°rio\n",
        "- Facilita debugging e an√°lise\n",
        "\n",
        "‚ö†Ô∏è **Aten√ß√£o:** Este processo pode demorar alguns minutos na primeira vez (modelo tem ~8GB)!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregando modelo e tokenizer\n",
        "print(\"üì• Carregando tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "print(\"üì• Carregando modelo (pode demorar alguns minutos)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,    # Usa menos mem√≥ria\n",
        "    device_map=\"auto\"             # Distribui automaticamente na GPU/CPU\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Modelo e tokenizer carregados com sucesso!\")\n",
        "print(f\"üìä Vocabul√°rio do tokenizer: {len(tokenizer.vocab):,} tokens\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéõÔ∏è Criando nosso LogitsProcessor\n",
        "\n",
        "Agora vamos criar nossa pr√≥pria classe para controlar a gera√ß√£o de texto!\n",
        "\n",
        "**Como funciona um LogitsProcessor:**\n",
        "1. üìä **Recebe logits** - Probabilidades brutas do modelo para cada palavra\n",
        "2. üîß **Modifica probabilidades** - Aplica nossas regras\n",
        "3. üì§ **Retorna logits modificados** - Para o modelo escolher a pr√≥xima palavra\n",
        "\n",
        "**Nossa implementa√ß√£o vai permitir:**\n",
        "- üìù **Definir regras** - Dicion√°rio com palavras e suas probabilidades\n",
        "- ‚ûï **Incentivar palavras** - Valores positivos aumentam chance\n",
        "- ‚ûñ **Bloquear palavras** - Valores negativos ou zero diminuem chance\n",
        "- üîÑ **Aplicar em tempo real** - Durante a gera√ß√£o do texto\n",
        "\n",
        "**Exemplo de regras:**\n",
        "```python\n",
        "regras = {\n",
        "    \"excelente\": 2.0,    # Incentiva\n",
        "    \"ruim\": -5.0,        # Desincentiva\n",
        "    \"terr√≠vel\": 0        # Bloqueia completamente\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classe para controlar a gera√ß√£o de texto\n",
        "class CustomLogitsProcessor(LogitsProcessor):\n",
        "    \"\"\"\n",
        "    Processor personalizado para aplicar regras na gera√ß√£o de texto\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, tokenizer, regras_dict: Dict[str, float]):\n",
        "        \"\"\"\n",
        "        Inicializa o processor com regras de masking\n",
        "        \n",
        "        Args:\n",
        "            tokenizer: Tokenizer do modelo\n",
        "            regras_dict: Dicion√°rio {palavra: peso}\n",
        "                       - peso > 0: incentiva a palavra\n",
        "                       - peso < 0: desincentiva a palavra  \n",
        "                       - peso = 0: bloqueia completamente\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token_adjustments = {}\n",
        "        \n",
        "        # Converte palavras em token IDs\n",
        "        for palavra, peso in regras_dict.items():\n",
        "            # Encode da palavra (pode gerar m√∫ltiplos tokens)\n",
        "            token_ids = tokenizer.encode(palavra, add_special_tokens=False)\n",
        "            \n",
        "            for token_id in token_ids:\n",
        "                # Se peso = 0, bloqueia completamente (probabilidade -infinito)\n",
        "                if peso == 0:\n",
        "                    self.token_adjustments[token_id] = float('-inf')\n",
        "                else:\n",
        "                    self.token_adjustments[token_id] = peso\n",
        "        \n",
        "        print(f\"‚úÖ Processor criado com {len(self.token_adjustments)} tokens ajustados\")\n",
        "    \n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Aplica as regras nos logits durante a gera√ß√£o\n",
        "        \n",
        "        Args:\n",
        "            input_ids: Tokens j√° gerados\n",
        "            scores: Probabilidades para pr√≥ximo token\n",
        "            \n",
        "        Returns:\n",
        "            scores modificados com nossas regras\n",
        "        \"\"\"\n",
        "        # Clona os scores para n√£o modificar o original\n",
        "        modified_scores = scores.clone()\n",
        "        \n",
        "        # Aplica ajustes para cada token que temos regras\n",
        "        for token_id, adjustment in self.token_adjustments.items():\n",
        "            if adjustment == float('-inf'):\n",
        "                # Bloqueia completamente\n",
        "                modified_scores[:, token_id] = float('-inf')\n",
        "            else:\n",
        "                # Adiciona peso (positivo ou negativo)\n",
        "                modified_scores[:, token_id] += adjustment\n",
        "        \n",
        "        return modified_scores\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîí Exemplo 1: Bloqueando Palavras Indesejadas\n",
        "\n",
        "Vamos testar nosso processor bloqueando palavras que demonstram impot√™ncia em atendimento ao cliente.\n",
        "\n",
        "**Cen√°rio:** Atendimento ao cliente com pedido atrasado\n",
        "- ‚ùå **Bloquear:** \"desculpe\", \"infelizmente\", \"lamento\"\n",
        "- ‚úÖ **Resultado esperado:** Respostas mais assertivas e focadas em solu√ß√µes\n",
        "\n",
        "**Por que isso √© √∫til?**\n",
        "- Melhora experi√™ncia do cliente (menos frustrante)\n",
        "- For√ßa o modelo a focar em solu√ß√µes\n",
        "- Mant√©m profissionalismo sem demonstrar impot√™ncia\n",
        "\n",
        "**Como funciona:**\n",
        "1. Definimos palavras com peso = 0 (bloqueio total)\n",
        "2. O processor marca esses tokens como probabilidade -‚àû\n",
        "3. O modelo nunca escolhe essas palavras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo 1: Bloqueando palavras de impot√™ncia\n",
        "print(\"üîí Exemplo: Bloqueando palavras que demonstram impot√™ncia\")\n",
        "\n",
        "# Definir regras - peso 0 = bloqueio total\n",
        "regras_bloqueio = {\n",
        "    \"desculpe\": 0,\n",
        "    \"infelizmente\": 0, \n",
        "    \"lamento\": 0,\n",
        "    \"perd√£o\": 0\n",
        "}\n",
        "\n",
        "# Criar processor com essas regras\n",
        "processor_bloqueio = CustomLogitsProcessor(tokenizer, regras_bloqueio)\n",
        "\n",
        "# Criar pipeline de gera√ß√£o\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Prompt de atendimento ao cliente\n",
        "prompt_atendimento = \"\"\"<|system|>\n",
        "Voc√™ √© um atendente de e-commerce focado em solu√ß√µes. Seja direto e profissional.\n",
        "\n",
        "Situa√ß√£o: Pedido #12345 est√° atrasado. Motivo: alto volume de entregas na regi√£o. Nova previs√£o: 2 dias √∫teis.\n",
        "<|user|>\n",
        "Meu pedido est√° atrasado e preciso dele urgente! N√£o aceito mais desculpas, quero uma solu√ß√£o!\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "# Gerar resposta COM bloqueio\n",
        "print(\"\\\\nüìù Gerando resposta com palavras bloqueadas...\")\n",
        "response_blocked = pipe(\n",
        "    prompt_atendimento,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    logits_processor=[processor_bloqueio],\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"\\\\n‚úÖ Resposta gerada:\")\n",
        "print(response_blocked[0][\"generated_text\"].split(\"<|assistant|>\")[-1].strip())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîÑ Compara√ß√£o: Com vs Sem Bloqueio\n",
        "\n",
        "Vamos gerar a mesma resposta SEM o bloqueio para ver a diferen√ßa!\n",
        "\n",
        "**Objetivo:** Demonstrar o impacto do Logits Masking comparando:\n",
        "- üö´ **Sem controle** - Modelo livre para usar qualquer palavra\n",
        "- ‚úÖ **Com controle** - Palavras indesejadas bloqueadas\n",
        "\n",
        "Esta compara√ß√£o mostra como pequenos ajustes podem ter grande impacto na experi√™ncia do usu√°rio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compara√ß√£o: Gerando SEM bloqueio\n",
        "print(\"üö´ Gerando resposta SEM bloqueio para compara√ß√£o...\")\n",
        "\n",
        "response_normal = pipe(\n",
        "    prompt_atendimento,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    # Sem logits_processor!\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"\\\\nüìä COMPARA√á√ÉO DE RESULTADOS:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\\\nüö´ SEM BLOQUEIO (modelo livre):\")\n",
        "print(response_normal[0][\"generated_text\"].split(\"<|assistant|>\")[-1].strip())\n",
        "\n",
        "print(\"\\\\n‚úÖ COM BLOQUEIO (palavras controladas):\")  \n",
        "print(response_blocked[0][\"generated_text\"].split(\"<|assistant|>\")[-1].strip())\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 60)\n",
        "print(\"üí° Observe a diferen√ßa no tom e nas palavras utilizadas!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ Exemplo 2: Incentivando Marcas Espec√≠ficas\n",
        "\n",
        "Agora vamos usar Logits Masking para influenciar recomenda√ß√µes de produtos!\n",
        "\n",
        "**Cen√°rio:** E-commerce de t√™nis esportivos\n",
        "- ‚úÖ **Incentivar:** New Balance (+3), Asics (+2) - parcerias comerciais\n",
        "- ‚ùå **Desincenttivar:** Nike (-2), Adidas (-2) - menor margem de lucro\n",
        "- üéØ **Objetivo:** Direcionar vendas para produtos mais lucrativos\n",
        "\n",
        "**Como funciona o incentivo:**\n",
        "- **Peso positivo** ‚Üí Aumenta probabilidade da palavra aparecer\n",
        "- **Peso negativo** ‚Üí Diminui probabilidade (mas n√£o bloqueia)\n",
        "- **Magnitude** ‚Üí Quanto maior o n√∫mero, maior o efeito\n",
        "\n",
        "**Aplica√ß√µes reais:**\n",
        "- üí∞ Produtos com maior margem\n",
        "- üì¶ Produtos em estoque\n",
        "- ü§ù Marcas parceiras\n",
        "- üî• Produtos em promo√ß√£o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo 2: Incentivando marcas espec√≠ficas\n",
        "print(\"üéØ Exemplo: Influenciando recomenda√ß√µes de t√™nis\")\n",
        "\n",
        "# Regras para influenciar marcas\n",
        "regras_marcas = {\n",
        "    \"New Balance\": 3.0,    # Forte incentivo (parceria comercial)\n",
        "    \"Asics\": 2.0,          # Incentivo moderado (boa margem)\n",
        "    \"Nike\": -2.0,          # Desincentivo (menor margem)\n",
        "    \"Adidas\": -2.0,        # Desincentivo (menor margem)\n",
        "    \"Puma\": 0.5,           # Leve incentivo (neutro)\n",
        "}\n",
        "\n",
        "# Criar processor para marcas\n",
        "processor_marcas = CustomLogitsProcessor(tokenizer, regras_marcas)\n",
        "\n",
        "# Prompt de recomenda√ß√£o de t√™nis\n",
        "prompt_tenis = \"\"\"<|system|>\n",
        "Voc√™ √© um assistente especializado em t√™nis esportivos. Recomende os melhores produtos do nosso cat√°logo considerando qualidade e custo-benef√≠cio.\n",
        "\n",
        "Cat√°logo dispon√≠vel: Nike, Adidas, Asics, New Balance, Puma, Under Armour, Reebok, Mizuno, Saucony, Brooks\n",
        "<|user|>\n",
        "Preciso de um t√™nis de corrida para longas dist√¢ncias. Quais s√£o suas melhores recomenda√ß√µes?\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "# Gerar recomenda√ß√£o COM influ√™ncia de marcas\n",
        "print(\"\\\\nüìù Gerando recomenda√ß√£o com marcas influenciadas...\")\n",
        "response_influenced = pipe(\n",
        "    prompt_tenis,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    logits_processor=[processor_marcas],\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"\\\\n‚úÖ Recomenda√ß√£o com marcas influenciadas:\")\n",
        "print(response_influenced[0][\"generated_text\"].split(\"<|assistant|>\")[-1].strip())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä Compara√ß√£o Final: Influ√™ncia vs Natural\n",
        "\n",
        "Vamos ver como o modelo recomenda naturalmente vs com nossa influ√™ncia!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compara√ß√£o: Recomenda√ß√£o natural vs influenciada\n",
        "print(\"üîÑ Gerando recomenda√ß√£o NATURAL para compara√ß√£o...\")\n",
        "\n",
        "response_natural = pipe(\n",
        "    prompt_tenis,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    # Sem logits_processor!\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"\\\\nüìä COMPARA√á√ÉO DE RECOMENDA√á√ïES:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\\\nüåø RECOMENDA√á√ÉO NATURAL (sem influ√™ncia):\")\n",
        "print(response_natural[0][\"generated_text\"].split(\"<|assistant|>\")[-1].strip())\n",
        "\n",
        "print(\"\\\\nüéØ RECOMENDA√á√ÉO INFLUENCIADA (com Logits Masking):\")\n",
        "print(response_influenced[0][\"generated_text\"].split(\"<|assistant|>\")[-1].strip())\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 80)\n",
        "print(\"üí° An√°lise:\")\n",
        "print(\"üìà Marcas incentivadas (New Balance, Asics) aparecem mais?\")\n",
        "print(\"üìâ Marcas desincentivadas (Nike, Adidas) aparecem menos?\")\n",
        "print(\"üéØ O modelo segue nossa estrat√©gia comercial?\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéì Conclus√µes e Pr√≥ximos Passos\n",
        "\n",
        "### ‚úÖ O que aprendemos:\n",
        "\n",
        "1. **Logits Masking √© poderoso** - Pequenos ajustes = Grande impacto\n",
        "2. **Controle granular** - Podemos incentivar/bloquear palavras espec√≠ficas  \n",
        "3. **Aplica√ß√£o pr√°tica** - √ötil para compliance, vendas, modera√ß√£o\n",
        "4. **F√°cil implementa√ß√£o** - Poucas linhas de c√≥digo para grandes mudan√ßas\n",
        "\n",
        "### üîß T√©cnicas demonstradas:\n",
        "\n",
        "- ‚ùå **Bloqueio total** (peso = 0) - Elimina palavras indesejadas\n",
        "- ‚ûï **Incentivo positivo** (peso > 0) - Aumenta probabilidade\n",
        "- ‚ûñ **Desincentivo** (peso < 0) - Diminui probabilidade\n",
        "- üîÑ **Aplica√ß√£o em tempo real** - Durante a gera√ß√£o\n",
        "\n",
        "### üöÄ Pr√≥ximos passos para experimentar:\n",
        "\n",
        "1. **Teste diferentes pesos** - Como -10, -1, 0.5, 5, 10 afetam os resultados?\n",
        "2. **M√∫ltiplas regras** - Combine bloqueio + incentivo no mesmo processor\n",
        "3. **Contexto din√¢mico** - Regras que mudam baseado no input do usu√°rio\n",
        "4. **Medi√ß√£o de impacto** - Analise quantitativamente as mudan√ßas\n",
        "5. **Casos de uso espec√≠ficos** - Adapte para seu dom√≠nio (finan√ßas, sa√∫de, etc.)\n",
        "\n",
        "### üí° Ideias para expandir:\n",
        "\n",
        "- **Sentiment control** - For√ßar tom positivo/neutro/profissional\n",
        "- **Product steering** - Direcionar para produtos espec√≠ficos\n",
        "- **Compliance enforcement** - Garantir respostas seguem regulamenta√ß√µes\n",
        "- **A/B testing** - Comparar diferentes estrat√©gias de masking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß™ √Årea de Experimenta√ß√£o\n",
        "print(\"üß™ Use esta c√©lula para experimentar com suas pr√≥prias regras!\")\n",
        "\n",
        "# Exemplo: Crie suas pr√≥prias regras aqui\n",
        "minhas_regras = {\n",
        "    # Adicione suas palavras e pesos aqui\n",
        "    # \"palavra\": peso\n",
        "    # Exemplos:\n",
        "    # \"excelente\": 2.0,    # Incentiva\n",
        "    # \"ruim\": -3.0,        # Desincentiva  \n",
        "    # \"terr√≠vel\": 0,       # Bloqueia\n",
        "}\n",
        "\n",
        "# Seu prompt personalizado aqui\n",
        "meu_prompt = \"\"\"<|system|>\n",
        "Escreva seu prompt aqui...\n",
        "<|user|>\n",
        "Sua pergunta aqui...\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "# Descomente para testar:\n",
        "\"\"\"\n",
        "if minhas_regras:  # S√≥ executa se voc√™ definir regras\n",
        "    meu_processor = CustomLogitsProcessor(tokenizer, minhas_regras)\n",
        "    \n",
        "    resultado = pipe(\n",
        "        meu_prompt,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        logits_processor=[meu_processor],\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    print(\"Resultado do seu experimento:\")\n",
        "    print(resultado[0][\"generated_text\"].split(\"<|assistant|>\")[-1].strip())\n",
        "\"\"\"\n",
        "\n",
        "print(\"üí° Dica: Modifique as regras acima e descomente o c√≥digo para testar!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelsonfrugeri-tech/playground-generative-ai/blob/master/logits_masking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMgIZyny7itpscklW8jjdzN",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
